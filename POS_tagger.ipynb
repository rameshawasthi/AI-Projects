{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO9RAjDlXKL7JwO6srx3+cB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshawasthi/AI-Projects/blob/main/POS_tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NlIVLMpH_Mm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import nltk\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function definitions\n",
        "## Download 10% of treebank\n",
        "nltk.download(\"treebank\")\n",
        "\n",
        "## Clear\n",
        "def clean_logs(data_dir):\n",
        "    logs_dir = os.path.join(data_dir, \"logs\")\n",
        "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
        "    return logs_dir\n",
        "\n",
        "## Get data\\n\",\n",
        "def download_and_read(dataset_dir, num_pairs=None):\n",
        "    sent_filename = os.path.join(dataset_dir, \"treebank-sentst.txt\")\n",
        "    poss_filename = os.path.join(dataset_dir, \"treebank-posst.txt\")\n",
        "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
        "        import nltk \n",
        "    \n",
        "            if not os.path.exists(dataset_dir):\n",
        "               os.makedirs(dataset_dir)\n",
        "           fsents = open(sent_filename, \"w\")\n",
        "           fposs = open(poss_filename, \"w\")\n",
        "           sentences = nltk.corpus.treebank.tagged_sents()\n",
        "           for sent in sentences:\n",
        "                fsents.write(\" \".join([w for w, p in sent]) + \"\\n\\\")\n",
        "                fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n",
        "    \n",
        "           fsents.close()\n",
        "           fposs.close()\n",
        "        sents, poss = [], []\n",
        "        with open(sent_filename, \"r\") as fsent:\n",
        "             for idx, line in enumerate(fsent):\n",
        "                 sents.append(line.strip())\\n\",\n",
        "                 if num_pairs is not None and idx >= num_pairs:\n",
        "                     break\n",
        "         with open(poss_filename, \"r\") as fposs:\n",
        "             for idx, line in enumerate(fposs):\n",
        "                 poss.append(line.strip())\n",
        "                 if num_pairs is not None and idx >= num_pairs:\n",
        "                     break\n",
        "         return sents, poss\n",
        "    \n",
        "## Tokenize build vocabulary\\n\",\n",
        "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
        "        if vocab_size is None:\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
        "        else:\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer\n",
        "                num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "        if vocab_size is not None:\n",
        "          # additional workaround, see issue 8092\\n\",\n",
        "          # https://github.com/keras-team/keras/issues/8092\\n\",\n",
        "            tokenizer.word_index = {e:i for e, i in tokenizer.word_index.items()\n",
        "                if i <= vocab_size+1 }\n",
        "        word2idx = tokenizer.word_index\n",
        "        idx2word = {v:k for k, v in word2idx.items()}\n",
        "        return word2idx, idx2word, tokenizer\n",
        "    \n",
        "    # Class\n",
        "class POSTaggingModel(tf.keras.Model):\n",
        "   def __init__(self, source_vocab_size, target_vocab_size,\n",
        "          embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
        "       super(POSTaggingModel, self).__init__(**kwargs)\n",
        "       self.embed = tf.keras.layers.Embedding(\n",
        "                source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
        "            self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
        "            self.rnn = tf.keras.layers.Bidirectional(\n",
        "                tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
        "            self.dense = tf.keras.layers.TimeDistributed(\n",
        "                tf.keras.layers.Dense(target_vocab_size))\n",
        "            self.activation = tf.keras.layers.Activation(\"softmax\")\n",
        "   \n",
        "    def call(self, x):\n",
        "            x = self.embed(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.rnn(x)\n",
        "            x = self.dense(x)\n",
        "            x = self.activation(x)\n",
        "            return x\n",
        "    \n",
        "# Custom metric for zeros\n",
        "    def masked_accuracy():\n",
        "        def masked_accuracy_fn(ytrue, ypred):\n",
        "            ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
        "            ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
        "    \n",
        "            mask = tf.keras.backend.cast()\n",
        "                tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
        "            matches = tf.keras.backend.cast(\n",
        "                tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
        "            numer = tf.keras.backend.sum(matches)\n",
        "            denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1),\n",
        "            accuracy =  numer / denom\n",
        "            return accuracy\n",
        "\n",
        "    return masked_accuracy_fn\n"
      ],
      "metadata": {
        "id": "3snqQkocInSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "cde6b62d-1c89-4728-833b-657924786888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    fsents = open(sent_filename, \"w\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables\n",
        "NUM_PAIRS = None\n",
        "EMBEDDING_DIM = 128\n",
        "RNN_OUTPUT_DIM = 256\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 50\n",
        "    \n",
        "# set random seed\\n\",\n",
        "tf.random.set_seed(42)\n",
        "    \n",
        "# clean up log area\\n\",\n",
        "data_dir = \"./datasets\"\n",
        "logs_dir = clean_logs(data_dir)\n",
        "    \n",
        "# download and read source and target data into data structure\n",
        "sents, poss = download_and_read(\\\"./datasets\\\", num_pairs=NUM_PAIRS)\n",
        "assert(len(sents) == len(poss))\n",
        "print(\\\"# of records: {:d}\\\".format(len(sents)))\n",
        "      \n",
        "# vocabulary sizes\\n\",\n",
        "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n",
        "    sents, vocab_size=9000)\\n\",\n",
        "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n",
        "    poss, vocab_size=38, lower=False)\\n\",\n",
        "source_vocab_size = len(word2idx_s)\\n\",\n",
        "target_vocab_size = len(word2idx_t)\\n\",\n",
        "print(\\\"vocab sizes (source): {:d}, (target): {:d}\\\".format(\n",
        "        source_vocab_size, target_vocab_size))\n",
        "    \n",
        "max_seqlen = 270\n",
        "   \n",
        "  "
      ],
      "metadata": {
        "id": "7rYIfkrsoxWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # create dataset\\\n",
        "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
        "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
        "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
        "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "   poss_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "       (sents_as_ints, poss_as_ints))\n",
        "idx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n",
        "poss_as_catints = []\n",
        "for p in poss_as_ints:\n",
        "    poss_as_catints.append(tf.keras.utils.to_categorical(p, \n",
        "        num_classes=target_vocab_size, dtype=\\\"int32\\\"))\n",
        "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    poss_as_catints, maxlen=max_seqlen)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "       (sents_as_ints, poss_as_catints))\n",
        "    \n",
        "# split into training, validation, and test datasets\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = len(sents) // 3\n",
        "val_size = (len(sents) - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "# create batches\n",
        "batch_size = BATCH_SIZE\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "E4CrkysBO80U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "embedding_dim = EMBEDDING_DIM\n",
        "rnn_output_dim = RNN_OUTPUT_DIM\n",
        "   \n",
        "model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
        "        embedding_dim, max_seqlen, rnn_output_dim)\n",
        "model.build(input_shape=(batch_size, max_seqlen))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\", masked_accuracy()])\n",
        "    \n",
        "# training\n",
        "num_epochs = NUM_EPOCHS\n",
        "\n",
        "best_m_file = os.path.join(data_dir, \\\"best_m.h5\\\")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "        best_m_file,\n",
        "        save_weights_only=True,\n",
        "        save_best_only=True)\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
        "history = model.fit(train_dataset\n",
        "        epochs=num_epochs,\n",
        "        validation_data=val_dataset,\n",
        "        callbacks=[checkpoint, tensorboard])\n",
        "    \n",
        "# evaluate \n",
        "best_m = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
        "       embedding_dim, max_seqlen, rnn_output_dim)\n",
        "best_m.build(input_shape=(batch_size, max_seqlen))\n",
        "best_m.load_weights(best_m_file)\n",
        "best_m.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=\"adam\",\n",
        "        metrics=[\"accuracy\", masked_accuracy()])\n",
        "    \n",
        "test_loss, test_acc, test_masked_acc = best_m.evaluate(test_dataset)\n",
        "print(\"test loss: {:.3f}, test accuracy: {:.3f}, masked test accuracy: {:.3f}\".format(\n",
        "        test_loss, test_acc, test_masked_acc))\n",
        "    \n",
        "# predict on batches\n",
        "labels, predictions = [], []\n",
        "is_first_batch = True\n",
        "accuracies = []\n",
        "    \n",
        "for test_batch in test_dataset:\n",
        "    inputs_b, outputs_b = test_batch\n",
        "    preds_b = best_m.predict(inputs_b)\n",
        "    preds_b = np.argmax(preds_b, axis=-1)\n",
        "    outputs_b = np.argmax(outputs_b.numpy(), axis=-1)\n",
        "    for i, (pred_l, output_l) in enumerate(zip(preds_b, outputs_b)):\n",
        "        assert(len(pred_l) == len(output_l))\n",
        "        pad_len = np.nonzero(output_l)[0][0]\n",
        "        acc = np.count_nonzero(\n",
        "            np.equal(\n",
        "                    output_l[pad_len:], pred_l[pad_len:]\n",
        "                )\n",
        "            ) / len(output_l[pad_len:])\n",
        "            accuracies.append(acc)\n",
        "            if is_first_batch:\n",
        "                words = [idx2word_s[x] for x in inputs_b.numpy()[i][pad_len:]]\n",
        "                postags_l = [idx2word_t[x] for x in output_l[pad_len:] if x > 0]\n",
        "                postags_p = [idx2word_t[x] for x in pred_l[pad_len:] if x > 0]\n",
        "                print(\"labeled  : {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p)\n",
        "                    for (w, p) in zip(words, postags_l)])))\n",
        "                print(\"predicted: {:s}\".format(\" \".join([\"{:s}/{:s}\".format(w, p)\n",
        "                    for (w, p) in zip(words, postags_p)])))\n",
        "                print(\" \")\n",
        "        is_first_batch = False\n",
        "    \n",
        "accuracy_score = np.mean(np.array(accuracies))\n",
        "print(\"pos tagging accuracy: {:.3f}\".format(accuracy_score))\n",
        "  "
      ],
      "metadata": {
        "id": "Bmk0D2CWRJy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}