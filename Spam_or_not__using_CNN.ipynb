{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ILCeW4CpNCCnmheWS1YahSP7mnbwW7fg",
      "authorship_tag": "ABX9TyP1hiMNbHyhjPNSGAcFNM3N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshawasthi/AI-Projects/blob/main/Spam_or_not__using_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEuajcXSJ7Me"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import shutil\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define function\n",
        "#get data\n",
        "def download_and_read(url):\n",
        "  local_file = url.split('/')[-1]\n",
        "  p = tf.keras.utils.get_file(local_file, url, \n",
        "      extract=True, cache_dir=\".\")\n",
        "  labels, texts = [], []\n",
        "  local_file = os.path.join(\"datasets\", \"SMSSpamcollection\")\n",
        "  with open(local_file, \"r\") as fin:\n",
        "    for line in fin:\n",
        "        label, test = line.strip().split('\\t')\n",
        "        labels.append(1 if label == \"spam\" else 0)\n",
        "        texts.append(text)\n",
        "  return texts, labels\n",
        "\n",
        "#Build embedding matrix\n",
        "def build_embedding_matrix(sequences, word2idx, embedding_dim, embedding_file):\n",
        "    if os.path.exists(embedding_file):\n",
        "      E = np.load(embedding_file)\n",
        "    else:\n",
        "       vocab_size = len(word2idx)\n",
        "       E = np.zeros((vocab_size, embedding_dim))\n",
        "       word_vectors = api.load(EMBEDDING_MODEL)\n",
        "       for word, idx in word2idx.items():\n",
        "         try:\n",
        "            E[idx] = word_vectors_word_vec(word)\n",
        "         except keyError: # word not embedding\n",
        "             pass\n",
        "        #Except IndexError : # UNKs are mapped to sed over VOCAB_SIZE as well as 1 \n",
        "         # pass\n",
        "       np.save(embedding_file, E)\n",
        "       return E\n",
        "# Model class\n",
        "class SpamClassifierModel ( tf.keras.Model):\n",
        "  def __init__(self, vocab_sz, embed_sz, input_length, num_filters, kernel_sz, output_sz, run_mode, embedding_weights, **kwargs):\n",
        "     super(SpamClassifierModel, self).__init__(**kwargs)\n",
        "     if run_mode == \"scratch\":\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz, input_length=input_lenght, trainable=True)\n",
        "     elif run_mode == \"vectorizer\":\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz, input_length=input_length, weights = [embedding_weights], trainable=False)\n",
        "     else:\n",
        "          self.embedding = tf.keras.layers.Embedding(vocab_sz, embed_sz, input_length=input_length, weights = [embedding_weights], trainable=True)\n",
        "     self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
        "     self.conv = tf.keras.layers.conv1D(filers=num_filters, kernel_size=kernel_sz, activation=\"relu\")\n",
        "     self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
        "     self.dense = tf.keras.layers.Dense(output_sz, activation=\"softmax\")\n",
        "  def call (self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.pool(x)\n",
        "    x = self.dense(x)\n",
        "    return(x)  \n"
      ],
      "metadata": {
        "id": "p-mMCssQK6xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize variables\n",
        "DATA_DIR = \"datasets\"\n",
        "DATA_DIR = \"/content/spammessage.txt\"\n",
        "\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
        "#DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "#DATASET_URL = \"/content/drive/MyDrive/ML_AI/SPAM_DETECTION/smsspamcollection/SMSSpamCollection\"\n",
        "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
        "EMBEDDING_DIM = 300\n",
        "NUM_CLASSES =2\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 3\n",
        "#Number of ham are 8x more than spam\n",
        "CLASS_WEIGHTS = { 0: 1, 1:8 }\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "#choices = \"scratch\", \"vectorizer\", \"finetuning\"\n",
        "run_mode = \"scratch\""
      ],
      "metadata": {
        "id": "gIW0FhX_fEkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare data\n",
        "texts, labels = download_and_read(DATASET_URL)\n",
        "\n",
        "\n",
        "#tokenize and pad text\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
        "num_records = len(text_sequences)\n",
        "max_seqlen = len(text_sequences[0])\n",
        "print(\"{:d} sentence, max length: {:d}\".format(num_records, max_seqlen))\n",
        "\n",
        "#Labels\n",
        "cat_labels = tf.keras.utils.to_categorical(labels, num_class=NUM_CLASSES)\n",
        "\n",
        "#Vocabulary\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "word2idx[\"PAD\"] = 0\n",
        "idx2word[0] = \"PAD\"\n",
        "vocab_size = len(word2idx)\n",
        "print(\"vocab size: {:d}\".format(vocab_size))\n",
        "\n",
        "#split dataset\n",
        "dataset = tf.data.Dataset.form_tensor_slices((text_sequences, cat_labels))\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = num_records // 4\n",
        "val_size = (num_records - test_size) // 10\n",
        "test_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = tain_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "3T4EKW3ViGEp",
        "outputId": "7d44e3d3-c804-4313-f7c4-fb7de13e2a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5db05c461f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#tokenize and pad text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'read' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding\n",
        "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM, EMBEDDING_NUMPY_FILE)\n",
        "print(\"Embedding matrix: \", E.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "JkoPszMfMIhR",
        "outputId": "943f96bc-d6d3-45b0-bcf9-99ea79781588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3a52e27ccfd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_NUMPY_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Embedding matrix: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_sequences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weight,\n",
        "from sklearn.utils import validation\n",
        "#model definittion\n",
        "cov_num_filters = 256\n",
        "conv_kernel_size = 3\n",
        "model = SpamClassifierModel(vocab_size, EMBEDDING_DIM, max_seqlen, conv_num_filters, conv_kernel_size, NUM_CLASSES,run_mode, E)\n",
        "model.build(input_shape=(None, max_seqlen))\n",
        "model.summary()\n",
        "\n",
        "#compile and train\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", matrix=[\"accuracy\"])\n",
        "\n",
        "#train model\n",
        "\n",
        "model.fit(train_dataset, epochs=NUM_EPOCHS, validation_data= val_dataset, class_weight=CLASS_WEIGHTS)\n",
        "\n",
        "#evaluate against test set\n",
        "labels, prediction = [], []\n",
        "for Xtest, Ytest in test_dataset:\n",
        "  ytest = model.predict_on_batch(Xtest)\n",
        "  ytest = np.argmax(Ytest, axis=1)\n",
        "  ytest_ = np.argmax(Ytest_, axis=1)\n",
        "  labels.extend(ytest.tolist())\n",
        "  predictions.extend(ytest.tolist())\n",
        "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels,predictions)))\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(labels,predictions))\n",
        "\n"
      ],
      "metadata": {
        "id": "zheK3UVOQxmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}